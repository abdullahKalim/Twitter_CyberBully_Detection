{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07576cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52dcd625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.2.0-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.2.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Collecting Cython==0.29.28\n",
      "  Downloading Cython-0.29.28-py2.py3-none-any.whl (983 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gensim) (1.20.1)\n",
      "Installing collected packages: smart-open, Cython, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.23\n",
      "    Uninstalling Cython-0.29.23:\n",
      "      Successfully uninstalled Cython-0.29.23\n",
      "Successfully installed Cython-0.29.28 gensim-4.2.0 smart-open-6.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59510a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Annotation</th>\n",
       "      <th>oh_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.74948705591165E+017</td>\n",
       "      <td>5.74948705591165E+017</td>\n",
       "      <td>@halalflaws @biebervalue @greenlinerzjm I read...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.71917888690393E+017</td>\n",
       "      <td>5.71917888690393E+017</td>\n",
       "      <td>@ShreyaBafna3 Now you idiots claim that people...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.90255841338601E+017</td>\n",
       "      <td>3.90255841338601E+017</td>\n",
       "      <td>RT @Mooseoftorment Call me sexist, but when I ...</td>\n",
       "      <td>sexism</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.68208850655916E+017</td>\n",
       "      <td>5.68208850655916E+017</td>\n",
       "      <td>@g0ssipsquirrelx Wrong, ISIS follows the examp...</td>\n",
       "      <td>racism</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.75596338802373E+017</td>\n",
       "      <td>5.75596338802373E+017</td>\n",
       "      <td>#mkr No No No No No No</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   index                     id  \\\n",
       "0  5.74948705591165E+017  5.74948705591165E+017   \n",
       "1  5.71917888690393E+017  5.71917888690393E+017   \n",
       "2  3.90255841338601E+017  3.90255841338601E+017   \n",
       "3  5.68208850655916E+017  5.68208850655916E+017   \n",
       "4  5.75596338802373E+017  5.75596338802373E+017   \n",
       "\n",
       "                                                Text Annotation  oh_label  \n",
       "0  @halalflaws @biebervalue @greenlinerzjm I read...       none       0.0  \n",
       "1  @ShreyaBafna3 Now you idiots claim that people...       none       0.0  \n",
       "2  RT @Mooseoftorment Call me sexist, but when I ...     sexism       1.0  \n",
       "3  @g0ssipsquirrelx Wrong, ISIS follows the examp...     racism       1.0  \n",
       "4                             #mkr No No No No No No       none       0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"C:\\\\Users\\\\hp\\\\Documents\\\\MGP\\\\CyberBullying Detection\\\\Dataset\\\\twitter_parsed_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "281caea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@VeraVanHorne @RusConCan @VICE Sorry, is it Olga, or Tanya, or Lena, or Marina. How's you little Putin sweat shop? http://t.co/stp5NmjZRY\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Text\"].sample(1).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44336cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to the url |url| and open it\n"
     ]
    }
   ],
   "source": [
    "sent=\"Go to the url http:\\\\helloabdullaj\\\\google.com and open it\"\n",
    "sent=re.sub(r\"\\S*https?:\\S*\",\"|url|\",sent)\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2983afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_case_lower(text):\n",
    "    sample = text\n",
    "    if(type(sample)==str):\n",
    "        sample = \" \".join([x.lower() for x in sample.split()])\n",
    "    \n",
    "    return sample\n",
    "\n",
    "def remove_url(text):\n",
    "    sample = text\n",
    "    sample = re.sub(r\"\\S*https?:\\S*\", '', sample) #links and urls\n",
    "    \n",
    "    return sample\n",
    "\n",
    "def remove_html(text):\n",
    "    sample = text\n",
    "    comp = re.compile(r'<.*?>')\n",
    "    sample = re.sub(comp, '', sample)\n",
    "    \n",
    "    return sample\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    sample = text\n",
    "    sample = re.sub('\\[.*?\\]', '', sample) #text between [square brackets]\n",
    "    sample = re.sub('\\(.*?\\)', '', sample) #text between (parenthesis)\n",
    "    sample = re.sub('[%s]' % re.escape(string.punctuation), '', sample) #punctuations\n",
    "    sample = re.sub(\"[''\"\"...]\", '', sample) #list of quotation marks\n",
    "    \n",
    "    return sample\n",
    "\n",
    "only_english = set(nltk.corpus.words.words())\n",
    "def remove_special_characters(text):\n",
    "    sample = text\n",
    "    sample = re.sub(r'\\n', ' ', sample) #new line character\n",
    "    sample = re.sub(r'\\\\n', ' ', sample) #new line character\n",
    "    sample = ' '.join([w for w in nltk.wordpunct_tokenize(sample) if w.lower() in only_english or not w.isalpha()]) #doesn't remove indian languages\n",
    "    sample = ' '.join(list(filter(lambda ele: re.search(\"[a-zA-Z\\s]+\", ele) is not None, sample.split()))) #languages other than english\n",
    "    sample = \" \".join([x.strip() for x in sample.split()])\n",
    "    \n",
    "    return sample\n",
    "\n",
    "def remove_hashtags_total(text):\n",
    "    sample = text\n",
    "    sample = re.sub('#', ' ', sample)\n",
    "    \n",
    "    return sample\n",
    "\n",
    "def remove_hashtags_only(text):\n",
    "    sample = text\n",
    "    sample = ' '.join([x for x in s.split() if not x.startswith('#')])\n",
    "    \n",
    "    return sample\n",
    "\n",
    "def remove_emojis(text):\n",
    "    sample = text\n",
    "    sample = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE).sub(r'', sample) #emojis and symbols\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9ff4dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"@DrBiden Ask 83 Joe about his thoughts <b> on packing </b> the Supreme Court.  He seem to have any after 50 years in politics. #JoeBiden ...more sinister\" \\\\n\\\\n#Covid19UK \\\\n#Covid #News \\\\n\\\\nJust enter \"empty testing stations\" 小 校 into your search engIne and https://t.co/omPuIJzVnl\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2d92e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@drbiden ask 83 joe about his thoughts <b> on packing </b> the supreme court.  he seem to have any after 50 years in politics. #joebiden ...more sinister\" \\\\n\\\\n#covid19uk \\\\n#covid #news \\\\n\\\\njust enter \"empty testing stations\" 胁芯懈屑懈 泻邪屑懈 into your search engine and https://t.co/ompuijzvnl'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_case_lower(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bea11fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@DrBiden Ask 83 Joe about his thoughts <b> on packing </b> the Supreme Court.  He seem to have any after 50 years in politics. #JoeBiden ...more sinister\" \\\\n\\\\n#Covid19UK \\\\n#Covid #News \\\\n\\\\nJust enter \"empty testing stations\" 小 校 into your search engIne and https://t.co/omPuIJzVnl'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emojis(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9babf7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@DrBiden Ask 83 Joe about his thoughts <b> on packing </b> the Supreme Court.  He seem to have any after 50 years in politics. #JoeBiden ...more sinister\" \\\\n\\\\n#Covid19UK \\\\n#Covid #News \\\\n\\\\nJust enter \"empty testing stations\" 小 校 into your search engIne and '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d4859c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@DrBiden Ask 83 Joe about his thoughts <b> on packing </b> the Supreme Court.  He seem to have any after 50 years in politics. ...more sinister\" \\\\n\\\\n#Covid19UK \\\\n#Covid \\\\n\\\\nJust enter \"empty testing stations\" 小 校 into your search engIne and https://t.co/omPuIJzVnl'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_hashtags_only(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db186ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(text):\n",
    "    sample=text\n",
    "    sample=change_case_lower(sample)\n",
    "    sample=remove_url(sample)\n",
    "    sample=remove_html(sample)\n",
    "    sample=remove_punctuations(sample)\n",
    "    sample=remove_special_characters(sample)\n",
    "    sample=remove_emojis(sample)\n",
    "    sample=remove_hashtags_total(sample)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25a45ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ask joe about his on the supreme court he seem to have any after in politics more sinister nncovid19uk news enter empty testing into your search engine and'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "912f2057",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Text\"]=df[\"Text\"].apply(lambda x: clean_data(x) if(type(x)==str) else x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f34c278e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Annotation</th>\n",
       "      <th>oh_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.74948705591165E+017</td>\n",
       "      <td>5.74948705591165E+017</td>\n",
       "      <td>i read them in change in meaning the history o...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.71917888690393E+017</td>\n",
       "      <td>5.71917888690393E+017</td>\n",
       "      <td>shreyabafna3 now you claim that people who tri...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.90255841338601E+017</td>\n",
       "      <td>3.90255841338601E+017</td>\n",
       "      <td>call me but when i go to an auto place id rath...</td>\n",
       "      <td>sexism</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.68208850655916E+017</td>\n",
       "      <td>5.68208850655916E+017</td>\n",
       "      <td>g0ssipsquirrelx wrong the example of and the e...</td>\n",
       "      <td>racism</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.75596338802373E+017</td>\n",
       "      <td>5.75596338802373E+017</td>\n",
       "      <td>no no no no no no</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   index                     id  \\\n",
       "0  5.74948705591165E+017  5.74948705591165E+017   \n",
       "1  5.71917888690393E+017  5.71917888690393E+017   \n",
       "2  3.90255841338601E+017  3.90255841338601E+017   \n",
       "3  5.68208850655916E+017  5.68208850655916E+017   \n",
       "4  5.75596338802373E+017  5.75596338802373E+017   \n",
       "\n",
       "                                                Text Annotation  oh_label  \n",
       "0  i read them in change in meaning the history o...       none       0.0  \n",
       "1  shreyabafna3 now you claim that people who tri...       none       0.0  \n",
       "2  call me but when i go to an auto place id rath...     sexism       1.0  \n",
       "3  g0ssipsquirrelx wrong the example of and the e...     racism       1.0  \n",
       "4                                  no no no no no no       none       0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10590263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235892"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(only_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8140d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "def remove_stop_words(text, cores = 2):\n",
    "    \n",
    "    sample = text\n",
    "    sample = sample.lower()\n",
    "    sample = [word for word in sample.split() if not word in stops]\n",
    "    sample = ' '.join(sample)\n",
    "    \n",
    "    return sample\n",
    "def get_wordnet_pos(word):\n",
    "    \n",
    "    treebank_tag = nltk.pos_tag([word])[0][1]\n",
    "    \n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "def lemma_clean_text(text, cores = 1):\n",
    " \n",
    "    sample = text\n",
    "    sample = sample.split(' ')\n",
    "    sample = [lemmatizer.lemmatize(word.lower(), get_wordnet_pos(word.lower())) for word in sample if(len(word)>0)]\n",
    "    sample =  ' '.join(sample)\n",
    "    \n",
    "    return sample\n",
    "stem=PorterStemmer()\n",
    "def stem_clean_text(text):\n",
    " \n",
    "    sample = text\n",
    "    sample = sample.split()\n",
    "    sample = [stem.stem(word) for word in sample]\n",
    "    sample = ' '.join(sample)\n",
    "    \n",
    "    return sample\n",
    "def correct_spelling(text):\n",
    "    \n",
    "    sample = text\n",
    "    sample = str(TextBlob(text).correct())\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ca82550",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=\"RT @WittySam: Colin's review of Kat &amp; Andre is equivalent of a teacher saying well done on writing your name in pencil. #mkr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00b1d9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @wittysam: colin's review kat &amp; andre equivalent teacher saying well done writing name pencil. #mkr\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"rt @wittysam: colin's review kat &amp; andre equivalent teacher say well do write name pencil. #mkr\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=remove_stop_words(s)\n",
    "print(s)\n",
    "lemma_clean_text(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6918c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text(text,stem=False,lemma=False,spell=False):\n",
    "    if lemma and stem:\n",
    "        raise Exception('Either stem or lemma can be true, not both!')\n",
    "        return text\n",
    "    sample=text\n",
    "    sample=remove_stop_words(sample)\n",
    "    if(stem):\n",
    "        sample=stem_clean_text(sample)\n",
    "    if(lemma):\n",
    "        sample=lemma_clean_text(sample)\n",
    "    if(spell):\n",
    "        sample=correct_spelling(sample)\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bbbc32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"it @wittysam: coin's review at &amp; andre equivalent teacher say well do write name pencil. #mr\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_text(s,lemma=True,spell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d7ac9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['correct_text'] = df['Text'].apply(lambda x: correct_text(x,stem=True,spell=True) if (type(x)==str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df91b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"C:\\\\Users\\\\hp\\\\Documents\\\\MGP\\\\CyberBullying Detection\\\\Dataset\\\\twitter_stemed_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b330f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Annotation</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>correct_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.74948705591165E+017</td>\n",
       "      <td>5.74948705591165E+017</td>\n",
       "      <td>i read them in change in meaning the history o...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>read change mean history slavery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.71917888690393E+017</td>\n",
       "      <td>5.71917888690393E+017</td>\n",
       "      <td>shreyabafna3 now you claim that people who tri...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>shreyabafna3 claim people try stop become terr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.90255841338601E+017</td>\n",
       "      <td>3.90255841338601E+017</td>\n",
       "      <td>call me but when i go to an auto place id rath...</td>\n",
       "      <td>sexism</td>\n",
       "      <td>1.0</td>\n",
       "      <td>call go auto place id rather talk guy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.68208850655916E+017</td>\n",
       "      <td>5.68208850655916E+017</td>\n",
       "      <td>g0ssipsquirrelx wrong the example of and the e...</td>\n",
       "      <td>racism</td>\n",
       "      <td>1.0</td>\n",
       "      <td>g0ssipsquirrelx wrong example exactly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.75596338802373E+017</td>\n",
       "      <td>5.75596338802373E+017</td>\n",
       "      <td>no no no no no no</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16846</th>\n",
       "      <td>5.75606766236475E+017</td>\n",
       "      <td>5.75606766236475E+017</td>\n",
       "      <td>feeling so sorry for the they should be safe a...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>feel sorry safe at go home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16847</th>\n",
       "      <td>5.72333822886326E+017</td>\n",
       "      <td>5.72333822886326E+017</td>\n",
       "      <td>pretty good were happy with well never eating ...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pretty good happy well never eat place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16848</th>\n",
       "      <td>5.72326950057845E+017</td>\n",
       "      <td>5.72326950057845E+017</td>\n",
       "      <td>lemon we please go just one season of without ...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>lemon pleas go one season without someone call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16849</th>\n",
       "      <td>5.74799612642357E+017</td>\n",
       "      <td>5.74799612642357E+017</td>\n",
       "      <td>you are too stupid to talk to blocked</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>stupid talk block</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16850</th>\n",
       "      <td>5.68826121153684E+017</td>\n",
       "      <td>5.68826121153684E+017</td>\n",
       "      <td>and before you protest that not mad theres not...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>protest mad there much reason you remain tehco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16851 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       index                     id  \\\n",
       "0      5.74948705591165E+017  5.74948705591165E+017   \n",
       "1      5.71917888690393E+017  5.71917888690393E+017   \n",
       "2      3.90255841338601E+017  3.90255841338601E+017   \n",
       "3      5.68208850655916E+017  5.68208850655916E+017   \n",
       "4      5.75596338802373E+017  5.75596338802373E+017   \n",
       "...                      ...                    ...   \n",
       "16846  5.75606766236475E+017  5.75606766236475E+017   \n",
       "16847  5.72333822886326E+017  5.72333822886326E+017   \n",
       "16848  5.72326950057845E+017  5.72326950057845E+017   \n",
       "16849  5.74799612642357E+017  5.74799612642357E+017   \n",
       "16850  5.68826121153684E+017  5.68826121153684E+017   \n",
       "\n",
       "                                                    Text Annotation oh_label  \\\n",
       "0      i read them in change in meaning the history o...       none      0.0   \n",
       "1      shreyabafna3 now you claim that people who tri...       none      0.0   \n",
       "2      call me but when i go to an auto place id rath...     sexism      1.0   \n",
       "3      g0ssipsquirrelx wrong the example of and the e...     racism      1.0   \n",
       "4                                      no no no no no no       none      0.0   \n",
       "...                                                  ...        ...      ...   \n",
       "16846  feeling so sorry for the they should be safe a...       none      0.0   \n",
       "16847  pretty good were happy with well never eating ...       none      0.0   \n",
       "16848  lemon we please go just one season of without ...       none      0.0   \n",
       "16849              you are too stupid to talk to blocked       none      0.0   \n",
       "16850  and before you protest that not mad theres not...       none      0.0   \n",
       "\n",
       "                                            correct_text  \n",
       "0                       read change mean history slavery  \n",
       "1      shreyabafna3 claim people try stop become terr...  \n",
       "2                  call go auto place id rather talk guy  \n",
       "3                  g0ssipsquirrelx wrong example exactly  \n",
       "4                                                         \n",
       "...                                                  ...  \n",
       "16846                         feel sorry safe at go home  \n",
       "16847             pretty good happy well never eat place  \n",
       "16848  lemon pleas go one season without someone call...  \n",
       "16849                                  stupid talk block  \n",
       "16850  protest mad there much reason you remain tehco...  \n",
       "\n",
       "[16851 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec11a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X=df.drop(\"oh_label\",axis=1)\n",
    "Y=df[\"oh_label\"]\n",
    "train_x,test_x,train_y,test_y=train_test_split(X,Y,test_size=0.3,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "729ba02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.fillna(\" \", axis=0, inplace=True)\n",
    "test_x.fillna(\" \", axis=0, inplace=True)\n",
    "train_y.fillna(0.0, inplace=True)\n",
    "test_y.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48fbf0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index           0\n",
       "id              0\n",
       "Text            0\n",
       "Annotation      0\n",
       "correct_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85ebcd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_x[\"Text\"].iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e112b1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word_to_Vect():\n",
    "    '''Function that returns word embedding, if passed list of sentences and size of vector'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def w2v(self, corpus, size):\n",
    "        \n",
    "        #tokenization and model preparation\n",
    "        tokenize_sent = [str(sent).split() for sent in corpus]\n",
    "        #creating vord2vec for every word in every sentence in corpus\n",
    "        self.w2v = word2vec.Word2Vec(sentences=tokenize_sent,vector_size=size, min_count=1)\n",
    "        \n",
    "        return self.w2v\n",
    "    \n",
    "    def transform(self, X_corpus, size):\n",
    "        \n",
    "        array_wordEmbed = []\n",
    "        for sent in X_corpus:\n",
    "            vec = np.zeros(size).reshape((1, size))\n",
    "            count = 0.\n",
    "            if sent == '':\n",
    "                a = vec\n",
    "            else:\n",
    "                sent = sent.split(' ')\n",
    "                for word in sent:\n",
    "                    vec += w2v[word].reshape((1,size))\n",
    "                    count +=1\n",
    "            if count !=0:\n",
    "                vec /= count\n",
    "            a = vec\n",
    "            array_wordEmbed.append(a)\n",
    "        return np.concatenate(tuple(array_wordEmbed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8bd37c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word_to_Vect().w2v(corpus = [sentence for sentence in X['correct_text']], size = len(X['Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81439ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('may', 0.9999833703041077),\n",
       " ('woman', 0.9999833106994629),\n",
       " ('lot', 0.9999832510948181),\n",
       " ('made', 0.9999831318855286),\n",
       " ('little', 0.9999831318855286),\n",
       " ('use', 0.9999830722808838),\n",
       " ('still', 0.999983012676239),\n",
       " ('thing', 0.999983012676239),\n",
       " ('away', 0.999983012676239),\n",
       " ('must', 0.999983012676239)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.similar_by_vector('fast')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e4f083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v = pd.DataFrame({word:w2v.wv[word] for sent in X['correct_text'] for word in str(sent).split()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffa2fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=pd.concat([X_w2v,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6ac9238",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2, x_test2, y_train2,y_test2 = train_test_split(X_w2v, Y, test_size=0.33, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08ce94f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>read</th>\n",
       "      <th>change</th>\n",
       "      <th>mean</th>\n",
       "      <th>history</th>\n",
       "      <th>slavery</th>\n",
       "      <th>shreyabafna3</th>\n",
       "      <th>claim</th>\n",
       "      <th>people</th>\n",
       "      <th>try</th>\n",
       "      <th>stop</th>\n",
       "      <th>...</th>\n",
       "      <th>jcsliva1999</th>\n",
       "      <th>pott</th>\n",
       "      <th>prosecution</th>\n",
       "      <th>lorry</th>\n",
       "      <th>overdramat</th>\n",
       "      <th>feat</th>\n",
       "      <th>apocalypse</th>\n",
       "      <th>rhoward617</th>\n",
       "      <th>rossbarnes9</th>\n",
       "      <th>msaleh14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5157</th>\n",
       "      <td>-0.014552</td>\n",
       "      <td>-0.006437</td>\n",
       "      <td>-0.012667</td>\n",
       "      <td>-0.009042</td>\n",
       "      <td>-0.009258</td>\n",
       "      <td>-0.000313</td>\n",
       "      <td>-0.007765</td>\n",
       "      <td>-0.015689</td>\n",
       "      <td>-0.016307</td>\n",
       "      <td>-0.013464</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.350693e-05</td>\n",
       "      <td>-1.781613e-04</td>\n",
       "      <td>-0.000338</td>\n",
       "      <td>-0.000341</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>-0.000561</td>\n",
       "      <td>-0.000374</td>\n",
       "      <td>-0.000127</td>\n",
       "      <td>-0.000136</td>\n",
       "      <td>-0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16382</th>\n",
       "      <td>0.018108</td>\n",
       "      <td>0.007978</td>\n",
       "      <td>0.015795</td>\n",
       "      <td>0.011227</td>\n",
       "      <td>0.011521</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.009580</td>\n",
       "      <td>0.019574</td>\n",
       "      <td>0.020443</td>\n",
       "      <td>0.016867</td>\n",
       "      <td>...</td>\n",
       "      <td>4.715468e-05</td>\n",
       "      <td>1.461526e-04</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>-0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>0.012071</td>\n",
       "      <td>0.005271</td>\n",
       "      <td>0.010449</td>\n",
       "      <td>0.007470</td>\n",
       "      <td>0.007717</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.006364</td>\n",
       "      <td>0.013011</td>\n",
       "      <td>0.013608</td>\n",
       "      <td>0.011187</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.607515e-07</td>\n",
       "      <td>7.989909e-05</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>-0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9322</th>\n",
       "      <td>-0.001397</td>\n",
       "      <td>-0.000648</td>\n",
       "      <td>-0.001196</td>\n",
       "      <td>-0.000868</td>\n",
       "      <td>-0.000877</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000702</td>\n",
       "      <td>-0.001556</td>\n",
       "      <td>-0.001608</td>\n",
       "      <td>-0.001329</td>\n",
       "      <td>...</td>\n",
       "      <td>5.441904e-05</td>\n",
       "      <td>-4.545852e-05</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15520</th>\n",
       "      <td>0.020351</td>\n",
       "      <td>0.008917</td>\n",
       "      <td>0.017543</td>\n",
       "      <td>0.012593</td>\n",
       "      <td>0.012915</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>0.010761</td>\n",
       "      <td>0.021843</td>\n",
       "      <td>0.022883</td>\n",
       "      <td>0.018831</td>\n",
       "      <td>...</td>\n",
       "      <td>3.204857e-05</td>\n",
       "      <td>2.499920e-04</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.000046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2623</th>\n",
       "      <td>-0.005108</td>\n",
       "      <td>-0.002254</td>\n",
       "      <td>-0.004411</td>\n",
       "      <td>-0.003215</td>\n",
       "      <td>-0.003288</td>\n",
       "      <td>-0.000114</td>\n",
       "      <td>-0.002673</td>\n",
       "      <td>-0.005373</td>\n",
       "      <td>-0.005673</td>\n",
       "      <td>-0.004675</td>\n",
       "      <td>...</td>\n",
       "      <td>8.475355e-06</td>\n",
       "      <td>4.553855e-07</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000126</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>-0.000175</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12363</th>\n",
       "      <td>-0.017914</td>\n",
       "      <td>-0.007885</td>\n",
       "      <td>-0.015515</td>\n",
       "      <td>-0.011183</td>\n",
       "      <td>-0.011458</td>\n",
       "      <td>-0.000337</td>\n",
       "      <td>-0.009571</td>\n",
       "      <td>-0.019278</td>\n",
       "      <td>-0.020145</td>\n",
       "      <td>-0.016660</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.169834e-05</td>\n",
       "      <td>-1.923576e-04</td>\n",
       "      <td>-0.000373</td>\n",
       "      <td>-0.000474</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000597</td>\n",
       "      <td>-0.000487</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000120</td>\n",
       "      <td>-0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5695</th>\n",
       "      <td>0.045674</td>\n",
       "      <td>0.020085</td>\n",
       "      <td>0.039550</td>\n",
       "      <td>0.028266</td>\n",
       "      <td>0.029183</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.024286</td>\n",
       "      <td>0.049073</td>\n",
       "      <td>0.051424</td>\n",
       "      <td>0.042335</td>\n",
       "      <td>...</td>\n",
       "      <td>1.211772e-04</td>\n",
       "      <td>5.108151e-04</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.001160</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8006</th>\n",
       "      <td>0.035466</td>\n",
       "      <td>0.015550</td>\n",
       "      <td>0.030751</td>\n",
       "      <td>0.022006</td>\n",
       "      <td>0.022622</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.018803</td>\n",
       "      <td>0.038155</td>\n",
       "      <td>0.039792</td>\n",
       "      <td>0.032859</td>\n",
       "      <td>...</td>\n",
       "      <td>1.330438e-04</td>\n",
       "      <td>3.386387e-04</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13151</th>\n",
       "      <td>-0.009076</td>\n",
       "      <td>-0.004022</td>\n",
       "      <td>-0.007878</td>\n",
       "      <td>-0.005582</td>\n",
       "      <td>-0.005776</td>\n",
       "      <td>-0.000206</td>\n",
       "      <td>-0.004887</td>\n",
       "      <td>-0.009866</td>\n",
       "      <td>-0.010217</td>\n",
       "      <td>-0.008491</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.125346e-05</td>\n",
       "      <td>-8.853468e-05</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>-0.000224</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>-0.000390</td>\n",
       "      <td>-0.000204</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000127</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11290 rows  6346 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           read    change      mean   history   slavery  shreyabafna3  \\\n",
       "5157  -0.014552 -0.006437 -0.012667 -0.009042 -0.009258     -0.000313   \n",
       "16382  0.018108  0.007978  0.015795  0.011227  0.011521      0.000415   \n",
       "362    0.012071  0.005271  0.010449  0.007470  0.007717      0.000257   \n",
       "9322  -0.001397 -0.000648 -0.001196 -0.000868 -0.000877     -0.000053   \n",
       "15520  0.020351  0.008917  0.017543  0.012593  0.012915      0.000344   \n",
       "...         ...       ...       ...       ...       ...           ...   \n",
       "2623  -0.005108 -0.002254 -0.004411 -0.003215 -0.003288     -0.000114   \n",
       "12363 -0.017914 -0.007885 -0.015515 -0.011183 -0.011458     -0.000337   \n",
       "5695   0.045674  0.020085  0.039550  0.028266  0.029183      0.000860   \n",
       "8006   0.035466  0.015550  0.030751  0.022006  0.022622      0.000677   \n",
       "13151 -0.009076 -0.004022 -0.007878 -0.005582 -0.005776     -0.000206   \n",
       "\n",
       "          claim    people       try      stop  ...   jcsliva1999  \\\n",
       "5157  -0.007765 -0.015689 -0.016307 -0.013464  ... -1.350693e-05   \n",
       "16382  0.009580  0.019574  0.020443  0.016867  ...  4.715468e-05   \n",
       "362    0.006364  0.013011  0.013608  0.011187  ... -6.607515e-07   \n",
       "9322  -0.000702 -0.001556 -0.001608 -0.001329  ...  5.441904e-05   \n",
       "15520  0.010761  0.021843  0.022883  0.018831  ...  3.204857e-05   \n",
       "...         ...       ...       ...       ...  ...           ...   \n",
       "2623  -0.002673 -0.005373 -0.005673 -0.004675  ...  8.475355e-06   \n",
       "12363 -0.009571 -0.019278 -0.020145 -0.016660  ... -5.169834e-05   \n",
       "5695   0.024286  0.049073  0.051424  0.042335  ...  1.211772e-04   \n",
       "8006   0.018803  0.038155  0.039792  0.032859  ...  1.330438e-04   \n",
       "13151 -0.004887 -0.009866 -0.010217 -0.008491  ... -6.125346e-05   \n",
       "\n",
       "               pott  prosecution     lorry  overdramat      feat  apocalypse  \\\n",
       "5157  -1.781613e-04    -0.000338 -0.000341   -0.000169 -0.000561   -0.000374   \n",
       "16382  1.461526e-04     0.000369  0.000475    0.000125  0.000663    0.000507   \n",
       "362    7.989909e-05     0.000232  0.000353    0.000089  0.000403    0.000311   \n",
       "9322  -4.545852e-05    -0.000044 -0.000010    0.000006 -0.000084   -0.000021   \n",
       "15520  2.499920e-04     0.000415  0.000512    0.000149  0.000753    0.000529   \n",
       "...             ...          ...       ...         ...       ...         ...   \n",
       "2623   4.553855e-07    -0.000044 -0.000126   -0.000023 -0.000146   -0.000175   \n",
       "12363 -1.923576e-04    -0.000373 -0.000474   -0.000117 -0.000597   -0.000487   \n",
       "5695   5.108151e-04     0.000909  0.001222    0.000420  0.001600    0.001160   \n",
       "8006   3.386387e-04     0.000737  0.000916    0.000373  0.001230    0.000889   \n",
       "13151 -8.853468e-05    -0.000160 -0.000224   -0.000100 -0.000390   -0.000204   \n",
       "\n",
       "       rhoward617  rossbarnes9  msaleh14  \n",
       "5157    -0.000127    -0.000136 -0.000048  \n",
       "16382    0.000082     0.000126 -0.000001  \n",
       "362      0.000113     0.000105 -0.000014  \n",
       "9322    -0.000053    -0.000064  0.000007  \n",
       "15520    0.000125     0.000190  0.000046  \n",
       "...           ...          ...       ...  \n",
       "2623     0.000017    -0.000006  0.000041  \n",
       "12363   -0.000071    -0.000120 -0.000042  \n",
       "5695     0.000278     0.000429  0.000060  \n",
       "8006     0.000242     0.000274  0.000065  \n",
       "13151   -0.000055    -0.000127  0.000014  \n",
       "\n",
       "[11290 rows x 6346 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ded82f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16851,)\n",
      "(16851, 5)\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25d911e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:03:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "clf=XGBClassifier()\n",
    "clf.fit(x_train2,y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a815be22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
