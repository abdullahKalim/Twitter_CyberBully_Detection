{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07576cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52dcd625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.2.0-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.2.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Collecting Cython==0.29.28\n",
      "  Downloading Cython-0.29.28-py2.py3-none-any.whl (983 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gensim) (1.20.1)\n",
      "Installing collected packages: smart-open, Cython, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.23\n",
      "    Uninstalling Cython-0.29.23:\n",
      "      Successfully uninstalled Cython-0.29.23\n",
      "Successfully installed Cython-0.29.28 gensim-4.2.0 smart-open-6.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59510a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Annotation</th>\n",
       "      <th>oh_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.74948705591165E+017</td>\n",
       "      <td>5.74948705591165E+017</td>\n",
       "      <td>@halalflaws @biebervalue @greenlinerzjm I read...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.71917888690393E+017</td>\n",
       "      <td>5.71917888690393E+017</td>\n",
       "      <td>@ShreyaBafna3 Now you idiots claim that people...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.90255841338601E+017</td>\n",
       "      <td>3.90255841338601E+017</td>\n",
       "      <td>RT @Mooseoftorment Call me sexist, but when I ...</td>\n",
       "      <td>sexism</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.68208850655916E+017</td>\n",
       "      <td>5.68208850655916E+017</td>\n",
       "      <td>@g0ssipsquirrelx Wrong, ISIS follows the examp...</td>\n",
       "      <td>racism</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.75596338802373E+017</td>\n",
       "      <td>5.75596338802373E+017</td>\n",
       "      <td>#mkr No No No No No No</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   index                     id  \\\n",
       "0  5.74948705591165E+017  5.74948705591165E+017   \n",
       "1  5.71917888690393E+017  5.71917888690393E+017   \n",
       "2  3.90255841338601E+017  3.90255841338601E+017   \n",
       "3  5.68208850655916E+017  5.68208850655916E+017   \n",
       "4  5.75596338802373E+017  5.75596338802373E+017   \n",
       "\n",
       "                                                Text Annotation  oh_label  \n",
       "0  @halalflaws @biebervalue @greenlinerzjm I read...       none       0.0  \n",
       "1  @ShreyaBafna3 Now you idiots claim that people...       none       0.0  \n",
       "2  RT @Mooseoftorment Call me sexist, but when I ...     sexism       1.0  \n",
       "3  @g0ssipsquirrelx Wrong, ISIS follows the examp...     racism       1.0  \n",
       "4                             #mkr No No No No No No       none       0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"C:\\\\Users\\\\hp\\\\Documents\\\\MGP\\\\CyberBullying Detection\\\\Dataset\\\\twitter_parsed_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "281caea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@VeraVanHorne @RusConCan @VICE Sorry, is it Olga, or Tanya, or Lena, or Marina. How's you little Putin sweat shop? http://t.co/stp5NmjZRY\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Text\"].sample(1).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44336cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to the url |url| and open it\n"
     ]
    }
   ],
   "source": [
    "sent=\"Go to the url http:\\\\helloabdullaj\\\\google.com and open it\"\n",
    "sent=re.sub(r\"\\S*https?:\\S*\",\"|url|\",sent)\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2983afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_case_lower(text):\n",
    "    sample = text\n",
    "    if(type(sample)==str):\n",
    "        sample = \" \".join([x.lower() for x in sample.split()])\n",
    "    \n",
    "    return sample\n",
    "\n",
    "def remove_url(text):\n",
    "    sample = text\n",
    "    sample = re.sub(r\"\\S*https?:\\S*\", '', sample) #links and urls\n",
    "    \n",
    "    return sample\n",
    "\n",
    "def remove_html(text):\n",
    "    sample = text\n",
    "    comp = re.compile(r'<.*?>')\n",
    "    sample = re.sub(comp, '', sample)\n",
    "    \n",
    "    return sample\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    sample = text\n",
    "    sample = re.sub('\\[.*?\\]', '', sample) #text between [square brackets]\n",
    "    sample = re.sub('\\(.*?\\)', '', sample) #text between (parenthesis)\n",
    "    sample = re.sub('[%s]' % re.escape(string.punctuation), '', sample) #punctuations\n",
    "    sample = re.sub(\"[''\"\"...]\", '', sample) #list of quotation marks\n",
    "    \n",
    "    return sample\n",
    "\n",
    "only_english = set(nltk.corpus.words.words())\n",
    "def remove_special_characters(text):\n",
    "    sample = text\n",
    "    sample = re.sub(r'\\n', ' ', sample) #new line character\n",
    "    sample = re.sub(r'\\\\n', ' ', sample) #new line character\n",
    "    sample = ' '.join([w for w in nltk.wordpunct_tokenize(sample) if w.lower() in only_english or not w.isalpha()]) #doesn't remove indian languages\n",
    "    sample = ' '.join(list(filter(lambda ele: re.search(\"[a-zA-Z\\s]+\", ele) is not None, sample.split()))) #languages other than english\n",
    "    sample = \" \".join([x.strip() for x in sample.split()])\n",
    "    \n",
    "    return sample\n",
    "\n",
    "def remove_hashtags_total(text):\n",
    "    sample = text\n",
    "    sample = re.sub('#', ' ', sample)\n",
    "    \n",
    "    return sample\n",
    "\n",
    "def remove_hashtags_only(text):\n",
    "    sample = text\n",
    "    sample = ' '.join([x for x in s.split() if not x.startswith('#')])\n",
    "    \n",
    "    return sample\n",
    "\n",
    "def remove_emojis(text):\n",
    "    sample = text\n",
    "    sample = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE).sub(r'', sample) #emojis and symbols\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9ff4dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"@DrBiden Ask 83 Joe about his thoughts <b> on packing </b> the Supreme Court.  He seem to have any after 50 years in politics. #JoeBiden ...more sinister\" \\\\n\\\\n#Covid19UK \\\\n#Covid #News \\\\n\\\\nJust enter \"empty testing stations\" 小 校 into your search engIne and https://t.co/omPuIJzVnl\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2d92e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@drbiden ask 83 joe about his thoughts <b> on packing </b> the supreme court.  he seem to have any after 50 years in politics. #joebiden ...more sinister\" \\\\n\\\\n#covid19uk \\\\n#covid #news \\\\n\\\\njust enter \"empty testing stations\" 胁芯懈屑懈 泻邪屑懈 into your search engine and https://t.co/ompuijzvnl'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_case_lower(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bea11fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@DrBiden Ask 83 Joe about his thoughts <b> on packing </b> the Supreme Court.  He seem to have any after 50 years in politics. #JoeBiden ...more sinister\" \\\\n\\\\n#Covid19UK \\\\n#Covid #News \\\\n\\\\nJust enter \"empty testing stations\" 小 校 into your search engIne and https://t.co/omPuIJzVnl'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emojis(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9babf7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@DrBiden Ask 83 Joe about his thoughts <b> on packing </b> the Supreme Court.  He seem to have any after 50 years in politics. #JoeBiden ...more sinister\" \\\\n\\\\n#Covid19UK \\\\n#Covid #News \\\\n\\\\nJust enter \"empty testing stations\" 小 校 into your search engIne and '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d4859c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@DrBiden Ask 83 Joe about his thoughts <b> on packing </b> the Supreme Court.  He seem to have any after 50 years in politics. ...more sinister\" \\\\n\\\\n#Covid19UK \\\\n#Covid \\\\n\\\\nJust enter \"empty testing stations\" 小 校 into your search engIne and https://t.co/omPuIJzVnl'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_hashtags_only(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db186ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(text):\n",
    "    sample=text\n",
    "    sample=change_case_lower(sample)\n",
    "    sample=remove_url(sample)\n",
    "    sample=remove_html(sample)\n",
    "    sample=remove_punctuations(sample)\n",
    "    sample=remove_special_characters(sample)\n",
    "    sample=remove_emojis(sample)\n",
    "    sample=remove_hashtags_total(sample)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25a45ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ask joe about his on the supreme court he seem to have any after in politics more sinister nncovid19uk news enter empty testing into your search engine and'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "912f2057",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Text\"]=df[\"Text\"].apply(lambda x: clean_data(x) if(type(x)==str) else x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f34c278e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Annotation</th>\n",
       "      <th>oh_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.74948705591165E+017</td>\n",
       "      <td>5.74948705591165E+017</td>\n",
       "      <td>i read them in change in meaning the history o...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.71917888690393E+017</td>\n",
       "      <td>5.71917888690393E+017</td>\n",
       "      <td>shreyabafna3 now you claim that people who tri...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.90255841338601E+017</td>\n",
       "      <td>3.90255841338601E+017</td>\n",
       "      <td>call me but when i go to an auto place id rath...</td>\n",
       "      <td>sexism</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.68208850655916E+017</td>\n",
       "      <td>5.68208850655916E+017</td>\n",
       "      <td>g0ssipsquirrelx wrong the example of and the e...</td>\n",
       "      <td>racism</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.75596338802373E+017</td>\n",
       "      <td>5.75596338802373E+017</td>\n",
       "      <td>no no no no no no</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   index                     id  \\\n",
       "0  5.74948705591165E+017  5.74948705591165E+017   \n",
       "1  5.71917888690393E+017  5.71917888690393E+017   \n",
       "2  3.90255841338601E+017  3.90255841338601E+017   \n",
       "3  5.68208850655916E+017  5.68208850655916E+017   \n",
       "4  5.75596338802373E+017  5.75596338802373E+017   \n",
       "\n",
       "                                                Text Annotation  oh_label  \n",
       "0  i read them in change in meaning the history o...       none       0.0  \n",
       "1  shreyabafna3 now you claim that people who tri...       none       0.0  \n",
       "2  call me but when i go to an auto place id rath...     sexism       1.0  \n",
       "3  g0ssipsquirrelx wrong the example of and the e...     racism       1.0  \n",
       "4                                  no no no no no no       none       0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10590263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235892"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(only_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8140d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "def remove_stop_words(text, cores = 2):\n",
    "    \n",
    "    sample = text\n",
    "    sample = sample.lower()\n",
    "    sample = [word for word in sample.split() if not word in stops]\n",
    "    sample = ' '.join(sample)\n",
    "    \n",
    "    return sample\n",
    "def get_wordnet_pos(word):\n",
    "    \n",
    "    treebank_tag = nltk.pos_tag([word])[0][1]\n",
    "    \n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "def lemma_clean_text(text, cores = 1):\n",
    " \n",
    "    sample = text\n",
    "    sample = sample.split(' ')\n",
    "    sample = [lemmatizer.lemmatize(word.lower(), get_wordnet_pos(word.lower())) for word in sample if(len(word)>0)]\n",
    "    sample =  ' '.join(sample)\n",
    "    \n",
    "    return sample\n",
    "stem=PorterStemmer()\n",
    "def stem_clean_text(text):\n",
    " \n",
    "    sample = text\n",
    "    sample = sample.split()\n",
    "    sample = [stem.stem(word) for word in sample]\n",
    "    sample = ' '.join(sample)\n",
    "    \n",
    "    return sample\n",
    "def correct_spelling(text):\n",
    "    \n",
    "    sample = text\n",
    "    sample = str(TextBlob(text).correct())\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ca82550",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=\"RT @WittySam: Colin's review of Kat &amp; Andre is equivalent of a teacher saying well done on writing your name in pencil. #mkr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00b1d9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @wittysam: colin's review kat &amp; andre equivalent teacher saying well done writing name pencil. #mkr\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"rt @wittysam: colin's review kat &amp; andre equivalent teacher say well do write name pencil. #mkr\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=remove_stop_words(s)\n",
    "print(s)\n",
    "lemma_clean_text(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6918c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text(text,stem=False,lemma=False,spell=False):\n",
    "    if lemma and stem:\n",
    "        raise Exception('Either stem or lemma can be true, not both!')\n",
    "        return text\n",
    "    sample=text\n",
    "    sample=remove_stop_words(sample)\n",
    "    if(stem):\n",
    "        sample=stem_clean_text(sample)\n",
    "    if(lemma):\n",
    "        sample=lemma_clean_text(sample)\n",
    "    if(spell):\n",
    "        sample=correct_spelling(sample)\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bbbc32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"it @wittysam: coin's review at &amp; andre equivalent teacher say well do write name pencil. #mr\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_text(s,lemma=True,spell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7ac9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['correct_text'] = df['Text'].apply(lambda x: correct_text(x,stem=True,spell=True) if (type(x)==str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b330f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec11a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X=df.drop(\"oh_label\",axis=1)\n",
    "Y=df[\"oh_label\"]\n",
    "train_x,test_x,train_y,test_y=train_test_split(X,Y,test_size=0.3,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "729ba02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.fillna(\" \", axis=0, inplace=True)\n",
    "test_x.fillna(\" \", axis=0, inplace=True)\n",
    "train_y.fillna(0.0, inplace=True)\n",
    "test_y.fillna(1.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48fbf0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index         0\n",
       "id            0\n",
       "Text          0\n",
       "Annotation    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85ebcd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_x[\"Text\"].iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e112b1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word_to_Vect():\n",
    "    '''Function that returns word embedding, if passed list of sentences and size of vector'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def w2v(self, corpus, size):\n",
    "        \n",
    "        #tokenization and model preparation\n",
    "        tokenize_sent = [str(sent).split() for sent in corpus]\n",
    "        #creating vord2vec for every word in every sentence in corpus\n",
    "        self.w2v = word2vec.Word2Vec(sentences=tokenize_sent,vector_size=size, min_count=1)\n",
    "        \n",
    "        return self.w2v\n",
    "    \n",
    "    def transform(self, X_corpus, size):\n",
    "        \n",
    "        array_wordEmbed = []\n",
    "        for sent in X_corpus:\n",
    "            vec = np.zeros(size).reshape((1, size))\n",
    "            count = 0.\n",
    "            if sent == '':\n",
    "                a = vec\n",
    "            else:\n",
    "                sent = sent.split(' ')\n",
    "                for word in sent:\n",
    "                    vec += w2v[word].reshape((1,size))\n",
    "                    count +=1\n",
    "            if count !=0:\n",
    "                vec /= count\n",
    "            a = vec\n",
    "            array_wordEmbed.append(a)\n",
    "        return np.concatenate(tuple(array_wordEmbed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8bd37c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word_to_Vect().w2v(corpus = [sentence for sentence in X['Text']], size = len(X['Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "81439ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pretty', 0.9999917149543762),\n",
       " ('mt89', 0.9999908804893494),\n",
       " ('bad', 0.9999894499778748),\n",
       " ('yes', 0.9999894499778748),\n",
       " ('feminism', 0.9999892711639404),\n",
       " ('w', 0.9999892711639404),\n",
       " ('literally', 0.9999889135360718),\n",
       " ('always', 0.9999886751174927),\n",
       " ('hell', 0.9999884366989136),\n",
       " ('anyone', 0.999988317489624)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.similar_by_vector('sound')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4e4f083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v = pd.DataFrame({word:w2v.wv[word] for sent in X['Text'] for word in str(sent).split()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffa2fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=pd.concat([X_w2v,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f6ac9238",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2, x_test2, y_train2,y_test2 = train_test_split(X_w2v, Y, test_size=0.33, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "08ce94f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>read</th>\n",
       "      <th>them</th>\n",
       "      <th>in</th>\n",
       "      <th>change</th>\n",
       "      <th>meaning</th>\n",
       "      <th>the</th>\n",
       "      <th>history</th>\n",
       "      <th>of</th>\n",
       "      <th>slavery</th>\n",
       "      <th>...</th>\n",
       "      <th>reb</th>\n",
       "      <th>lorry</th>\n",
       "      <th>accomplishment</th>\n",
       "      <th>overdramatic</th>\n",
       "      <th>feat</th>\n",
       "      <th>apocalypse</th>\n",
       "      <th>rhoward617</th>\n",
       "      <th>rossbarnes9</th>\n",
       "      <th>msaleh14</th>\n",
       "      <th>pacify</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5157</th>\n",
       "      <td>-0.035911</td>\n",
       "      <td>-0.028499</td>\n",
       "      <td>-0.036625</td>\n",
       "      <td>-0.041144</td>\n",
       "      <td>-0.014614</td>\n",
       "      <td>-0.007075</td>\n",
       "      <td>-0.039735</td>\n",
       "      <td>-0.018379</td>\n",
       "      <td>-0.040964</td>\n",
       "      <td>-0.018335</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000494</td>\n",
       "      <td>-0.000367</td>\n",
       "      <td>-0.000327</td>\n",
       "      <td>-0.000282</td>\n",
       "      <td>-0.000234</td>\n",
       "      <td>-0.000176</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>-0.000270</td>\n",
       "      <td>-0.000379</td>\n",
       "      <td>-0.000869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16382</th>\n",
       "      <td>0.050947</td>\n",
       "      <td>0.036421</td>\n",
       "      <td>0.047036</td>\n",
       "      <td>0.049988</td>\n",
       "      <td>0.018546</td>\n",
       "      <td>0.008906</td>\n",
       "      <td>0.047144</td>\n",
       "      <td>0.023051</td>\n",
       "      <td>0.048553</td>\n",
       "      <td>0.022906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.001086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>0.023889</td>\n",
       "      <td>0.014961</td>\n",
       "      <td>0.018680</td>\n",
       "      <td>0.018244</td>\n",
       "      <td>0.007536</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.017009</td>\n",
       "      <td>0.009193</td>\n",
       "      <td>0.016827</td>\n",
       "      <td>0.008942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9322</th>\n",
       "      <td>-0.010417</td>\n",
       "      <td>-0.005850</td>\n",
       "      <td>-0.006856</td>\n",
       "      <td>-0.007094</td>\n",
       "      <td>-0.002823</td>\n",
       "      <td>-0.001375</td>\n",
       "      <td>-0.005416</td>\n",
       "      <td>-0.003521</td>\n",
       "      <td>-0.006379</td>\n",
       "      <td>-0.003259</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>-0.000081</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>-0.000201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15520</th>\n",
       "      <td>0.021344</td>\n",
       "      <td>0.013747</td>\n",
       "      <td>0.017189</td>\n",
       "      <td>0.017924</td>\n",
       "      <td>0.006988</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.015632</td>\n",
       "      <td>0.008624</td>\n",
       "      <td>0.016381</td>\n",
       "      <td>0.008372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2623</th>\n",
       "      <td>0.007897</td>\n",
       "      <td>0.007373</td>\n",
       "      <td>0.009282</td>\n",
       "      <td>0.011293</td>\n",
       "      <td>0.003714</td>\n",
       "      <td>0.001926</td>\n",
       "      <td>0.010784</td>\n",
       "      <td>0.004796</td>\n",
       "      <td>0.011754</td>\n",
       "      <td>0.004919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12363</th>\n",
       "      <td>-0.030504</td>\n",
       "      <td>-0.022667</td>\n",
       "      <td>-0.029891</td>\n",
       "      <td>-0.033288</td>\n",
       "      <td>-0.011668</td>\n",
       "      <td>-0.005643</td>\n",
       "      <td>-0.031877</td>\n",
       "      <td>-0.014780</td>\n",
       "      <td>-0.033279</td>\n",
       "      <td>-0.014838</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000375</td>\n",
       "      <td>-0.000306</td>\n",
       "      <td>-0.000237</td>\n",
       "      <td>-0.000177</td>\n",
       "      <td>-0.000126</td>\n",
       "      <td>-0.000166</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>-0.000120</td>\n",
       "      <td>-0.000242</td>\n",
       "      <td>-0.000616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5695</th>\n",
       "      <td>0.083671</td>\n",
       "      <td>0.060263</td>\n",
       "      <td>0.077332</td>\n",
       "      <td>0.084527</td>\n",
       "      <td>0.030741</td>\n",
       "      <td>0.014930</td>\n",
       "      <td>0.079795</td>\n",
       "      <td>0.038525</td>\n",
       "      <td>0.082372</td>\n",
       "      <td>0.038298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.001706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8006</th>\n",
       "      <td>0.051459</td>\n",
       "      <td>0.034893</td>\n",
       "      <td>0.044709</td>\n",
       "      <td>0.047829</td>\n",
       "      <td>0.017674</td>\n",
       "      <td>0.008597</td>\n",
       "      <td>0.045931</td>\n",
       "      <td>0.022156</td>\n",
       "      <td>0.046789</td>\n",
       "      <td>0.022031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.000994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13151</th>\n",
       "      <td>0.026742</td>\n",
       "      <td>0.020564</td>\n",
       "      <td>0.027046</td>\n",
       "      <td>0.030380</td>\n",
       "      <td>0.010662</td>\n",
       "      <td>0.005236</td>\n",
       "      <td>0.031325</td>\n",
       "      <td>0.013531</td>\n",
       "      <td>0.031465</td>\n",
       "      <td>0.013529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11290 rows  9260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              i      read      them        in    change   meaning       the  \\\n",
       "5157  -0.035911 -0.028499 -0.036625 -0.041144 -0.014614 -0.007075 -0.039735   \n",
       "16382  0.050947  0.036421  0.047036  0.049988  0.018546  0.008906  0.047144   \n",
       "362    0.023889  0.014961  0.018680  0.018244  0.007536  0.003563  0.017009   \n",
       "9322  -0.010417 -0.005850 -0.006856 -0.007094 -0.002823 -0.001375 -0.005416   \n",
       "15520  0.021344  0.013747  0.017189  0.017924  0.006988  0.003400  0.015632   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "2623   0.007897  0.007373  0.009282  0.011293  0.003714  0.001926  0.010784   \n",
       "12363 -0.030504 -0.022667 -0.029891 -0.033288 -0.011668 -0.005643 -0.031877   \n",
       "5695   0.083671  0.060263  0.077332  0.084527  0.030741  0.014930  0.079795   \n",
       "8006   0.051459  0.034893  0.044709  0.047829  0.017674  0.008597  0.045931   \n",
       "13151  0.026742  0.020564  0.027046  0.030380  0.010662  0.005236  0.031325   \n",
       "\n",
       "        history        of   slavery  ...       reb     lorry  accomplishment  \\\n",
       "5157  -0.018379 -0.040964 -0.018335  ... -0.000494 -0.000367       -0.000327   \n",
       "16382  0.023051  0.048553  0.022906  ...  0.000681  0.000552        0.000376   \n",
       "362    0.009193  0.016827  0.008942  ...  0.000270  0.000176        0.000173   \n",
       "9322  -0.003521 -0.006379 -0.003259  ... -0.000119 -0.000044       -0.000018   \n",
       "15520  0.008624  0.016381  0.008372  ...  0.000277  0.000147        0.000173   \n",
       "...         ...       ...       ...  ...       ...       ...             ...   \n",
       "2623   0.004796  0.011754  0.004919  ...  0.000101  0.000160        0.000042   \n",
       "12363 -0.014780 -0.033279 -0.014838  ... -0.000375 -0.000306       -0.000237   \n",
       "5695   0.038525  0.082372  0.038298  ...  0.001120  0.000820        0.000674   \n",
       "8006   0.022156  0.046789  0.022031  ...  0.000668  0.000449        0.000401   \n",
       "13151  0.013531  0.031465  0.013529  ...  0.000356  0.000274        0.000233   \n",
       "\n",
       "       overdramatic      feat  apocalypse  rhoward617  rossbarnes9  msaleh14  \\\n",
       "5157      -0.000282 -0.000234   -0.000176   -0.000113    -0.000270 -0.000379   \n",
       "16382      0.000347  0.000263    0.000208    0.000208     0.000311  0.000426   \n",
       "362        0.000122  0.000150    0.000013    0.000095     0.000170  0.000152   \n",
       "9322       0.000002 -0.000018   -0.000045   -0.000079    -0.000081 -0.000124   \n",
       "15520      0.000141  0.000117    0.000026    0.000089     0.000098  0.000106   \n",
       "...             ...       ...         ...         ...          ...       ...   \n",
       "2623       0.000042  0.000101    0.000050    0.000037     0.000069  0.000073   \n",
       "12363     -0.000177 -0.000126   -0.000166   -0.000103    -0.000120 -0.000242   \n",
       "5695       0.000628  0.000474    0.000342    0.000431     0.000491  0.000767   \n",
       "8006       0.000341  0.000241    0.000132    0.000282     0.000233  0.000356   \n",
       "13151      0.000163  0.000178    0.000160    0.000089     0.000110  0.000226   \n",
       "\n",
       "         pacify  \n",
       "5157  -0.000869  \n",
       "16382  0.001086  \n",
       "362    0.000474  \n",
       "9322  -0.000201  \n",
       "15520  0.000380  \n",
       "...         ...  \n",
       "2623   0.000228  \n",
       "12363 -0.000616  \n",
       "5695   0.001706  \n",
       "8006   0.000994  \n",
       "13151  0.000562  \n",
       "\n",
       "[11290 rows x 9260 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ded82f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16851,)\n",
      "(16851, 4)\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25d911e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:34:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "clf=XGBClassifier()\n",
    "clf.fit(x_train2,y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a815be22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
